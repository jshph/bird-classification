{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the locations of the parts that are labeled on each image, and join them with the address of the corresponding images. This is to be able to train a classifier to distinguish between each `part_id` by looking at that part in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in part locations\n",
    "part_locs = pd.read_csv('CUB_200_2011/CUB_200_2011/parts/part_locs.txt', sep=' ', header=None)\n",
    "part_locs.columns = ['image_id', 'part_id', 'x', 'y', 'visible']\n",
    "\n",
    "# read in image ids and their addresses\n",
    "im_id_addrs = pd.read_csv('CUB_200_2011/CUB_200_2011/images.txt', sep=' ', header=None)\n",
    "im_id_addrs.columns = ['image_id', 'image_addr']\n",
    "\n",
    "im_addr_parts = im_id_addrs\n",
    "# join part locations and image id dataframes in order to get (part, image address) mappings\n",
    "# im_addr_parts = pd.merge(part_locs, im_id_addrs, on='image_id')\n",
    "\n",
    "# read in bounding boxes per image id\n",
    "im_bbs = pd.read_csv('CUB_200_2011/CUB_200_2011/bounding_boxes.txt', sep=' ', header=None)\n",
    "im_bbs.columns = ['image_id', 'bb_x', 'bb_y', 'bb_w', 'bb_h']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_bbs, on='image_id')\n",
    "\n",
    "im_classes = pd.read_csv('CUB_200_2011/CUB_200_2011/image_class_labels.txt', sep=' ', header=None)\n",
    "im_classes.columns = ['image_id', 'class_id']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_classes, on='image_id')\n",
    "\n",
    "# read in part labels manually because parts can be two words\n",
    "# part_labels = []\n",
    "# with open('CUB_200_2011/CUB_200_2011/parts/parts.txt') as part_labels_file:\n",
    "#     for row in part_labels_file:\n",
    "#         sep = row.find(' ')\n",
    "#         part_id, part_name = row[:sep], row[sep + 1:-1]\n",
    "#         part_labels.append({'part_id': int(part_id), 'part_name': part_name})\n",
    "# part_labels = pd.DataFrame.from_dict(part_labels)\n",
    "\n",
    "# join (part, image addresses) with part labels to make part_id's comprehensible\n",
    "# im_addr_parts = pd.merge(im_addr_parts, part_labels, on='part_id')\n",
    "print(im_addr_parts.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-039e1ba4dd63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# prev_class_id = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mim_addr_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mim_addr_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mim_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_addr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 20 # limit classes trained\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from skimage import io, transform\n",
    "\n",
    "# GENERATE THE TRAINING DATA\n",
    "\n",
    "# CLASS_IMGS = defaultdict(list)\n",
    "# first level key = class_id. second level key = part_id\n",
    "\n",
    "class_ids = set()\n",
    "\n",
    "CLASS_INSTANCES = []\n",
    "IMG_LABELS = []\n",
    "# prev_class_id = 0\n",
    "\n",
    "im_addr_classes = pickle.load(open('metadata', 'rb'))\n",
    "for index, row in im_addr_classes.iterrows():\n",
    "    im_id, im_addr, bb_x, bb_y, bb_w, bb_h, class_id = row\n",
    "    \n",
    "    if class_id not in class_ids:\n",
    "        if len(class_ids) >= NUM_CLASSES:\n",
    "            break\n",
    "        class_ids.add(class_id)\n",
    "    \n",
    "    img = io.imread('CUB_200_2011/images/' + im_addr)\n",
    "    \n",
    "    bb_x, bb_y, bb_w, bb_h = [int(x) for x in [bb_x, bb_y, bb_w, bb_h]]\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.dstack([img, img, img])\n",
    "    if 0 in img.shape:\n",
    "        continue\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    break\n",
    "        \n",
    "    img = img[bb_y:bb_y + bb_h, bb_x:bb_x + bb_w, :]\n",
    "    img = transform.resize(img, (224, 224, 3))\n",
    "\n",
    "\n",
    "#     print('extracting foreground for', class_id, im_addr)\n",
    "    \n",
    "    CLASS_INSTANCES.append(img)\n",
    "    IMG_LABELS.append(class_id)\n",
    "\n",
    "print(np.array(CLASS_INSTANCES).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON\"T RUN THIS\n",
    "\n",
    "all_imgs_processed = []\n",
    "all_labels = []\n",
    "for class_id, filename in enumerate(filenames):\n",
    "    class_imgs = pickle.load(open(filename, 'rb'))\n",
    "    for img_idx, img in enumerate(class_imgs):\n",
    "#         crops = []\n",
    "#         actual_num_crops = 0\n",
    "#         for i in range(NUM_CROPS):\n",
    "#             rand_x = randrange(0, img.shape[0] - CROP_SIZE + 1)\n",
    "#             rand_y = randrange(0, img.shape[1] - CROP_SIZE + 1)\n",
    "#             crop = img[rand_y:rand_y + CROP_SIZE, rand_x:rand_x + CROP_SIZE]\n",
    "#             if crop.shape != (100, 100, 3):\n",
    "#                 continue\n",
    "#             crops.append(crop)\n",
    "#             actual_num_crops += 1\n",
    "#         all_imgs_processed += crops\n",
    "#         all_labels += [class_id] * actual_num_crops\n",
    "        x_center = img.shape[0] // 2\n",
    "        y_center = img.shape[1] // 2\n",
    "        crop = img[y_center - CROP_SIZE//2:y_center + CROP_SIZE//2, x_center - CROP_SIZE//2:x_center + CROP_SIZE//2]\n",
    "        if crop.shape != (100, 100, 3):\n",
    "            continue\n",
    "        all_imgs_processed += [crop]\n",
    "        all_labels += [class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_processed = np.array(CLASS_INSTANCES)\n",
    "all_labels = np.array(IMG_LABELS)\n",
    "# np.savez_compressed('images_normalized', all_imgs_processed)\n",
    "# np.savez_compressed('images_normalize_labels', all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# VGGNet training\n",
    "\n",
    "# all_imgs_processed = np.load('images_normalized.npz')\n",
    "# all_labels = np.load('images_normalized_labels.npz')\n",
    "import vgg as vgg_mod\n",
    "import importlib\n",
    "importlib.reload(vgg_mod)\n",
    "\n",
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg_obj = vgg_mod.vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "\n",
    "labels_ph = tf.placeholder(tf.int64, (None), name='labels_ph')\n",
    "labels_ph_oh = tf.one_hot(labels_ph, 20)\n",
    "print(labels_ph_oh)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=vgg_obj.logits, labels=labels_ph_oh)\n",
    "optimizer = tf.train.AdamOptimizer(0.0001, 0.9, 0.999)\n",
    "opt = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(vgg_obj.probs, 1), labels_ph)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# # Unused prediction code\n",
    "\n",
    "# prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "\n",
    "# preds = (np.argsort(prob)[::-1])[0:5]\n",
    "# for p in preds:\n",
    "#     print class_names[p], prob[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n"
     ]
    }
   ],
   "source": [
    "# init = tf.initialize_variables([vgg_obj.fc1, vgg_obj.fc2, vgg_obj.fc31])\n",
    "# print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'fc31'))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vgg_obj.load_weights(vgg_obj.weights, vgg_obj.sess)\n",
    "# for varname in ['fc1', 'fc2', 'fc31']:\n",
    "#     coll = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'fc31')\n",
    "#     for var in coll:\n",
    "#         sess.run(var.initializer)\n",
    "# # sess.run(init)\n",
    "# sess.run(sess.graph.get_tensor_by_name('beta1_power/Assign:0'))\n",
    "# sess.run(sess.graph.get_tensor_by_name('beta2_power/Assign:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 224, 224, 3) (1115,)\n",
      "[  0] Accuracy: 0.065  \t  Loss: 0.807 \n",
      "[  1] Accuracy: 0.121  \t  Loss: 0.790 \n",
      "[  2] Accuracy: 0.133  \t  Loss: 0.781 \n",
      "[  3] Accuracy: 0.139  \t  Loss: 0.781 \n",
      "[  4] Accuracy: 0.136  \t  Loss: 0.780 \n",
      "[  5] Accuracy: 0.149  \t  Loss: 0.775 \n",
      "[  6] Accuracy: 0.128  \t  Loss: 0.777 \n",
      "[  7] Accuracy: 0.138  \t  Loss: 0.778 \n",
      "[  8] Accuracy: 0.115  \t  Loss: 0.784 \n",
      "[  9] Accuracy: 0.154  \t  Loss: 0.778 \n",
      "[ 10] Accuracy: 0.142  \t  Loss: 0.774 \n",
      "[ 11] Accuracy: 0.160  \t  Loss: 0.776 \n",
      "[ 12] Accuracy: 0.167  \t  Loss: 0.770 \n",
      "[ 13] Accuracy: 0.181  \t  Loss: 0.769 \n",
      "[ 14] Accuracy: 0.164  \t  Loss: 0.770 \n",
      "[ 15] Accuracy: 0.171  \t  Loss: 0.768 \n",
      "[ 16] Accuracy: 0.165  \t  Loss: 0.768 \n",
      "[ 17] Accuracy: 0.188  \t  Loss: 0.760 \n",
      "[ 18] Accuracy: 0.183  \t  Loss: 0.767 \n",
      "[ 19] Accuracy: 0.184  \t  Loss: 0.764 \n",
      "[ 20] Accuracy: 0.185  \t  Loss: 0.764 \n",
      "[ 21] Accuracy: 0.201  \t  Loss: 0.765 \n",
      "[ 22] Accuracy: 0.196  \t  Loss: 0.766 \n",
      "[ 23] Accuracy: 0.189  \t  Loss: 0.766 \n",
      "[ 24] Accuracy: 0.183  \t  Loss: 0.761 \n",
      "[ 25] Accuracy: 0.197  \t  Loss: 0.764 \n",
      "[ 26] Accuracy: 0.176  \t  Loss: 0.770 \n",
      "[ 27] Accuracy: 0.190  \t  Loss: 0.766 \n",
      "[ 28] Accuracy: 0.176  \t  Loss: 0.766 \n",
      "[ 29] Accuracy: 0.194  \t  Loss: 0.761 \n",
      "[ 30] Accuracy: 0.188  \t  Loss: 0.762 \n",
      "[ 31] Accuracy: 0.166  \t  Loss: 0.767 \n",
      "[ 32] Accuracy: 0.132  \t  Loss: 0.777 \n",
      "[ 33] Accuracy: 0.149  \t  Loss: 0.777 \n",
      "[ 34] Accuracy: 0.154  \t  Loss: 0.774 \n",
      "[ 35] Accuracy: 0.181  \t  Loss: 0.769 \n",
      "[ 36] Accuracy: 0.177  \t  Loss: 0.765 \n",
      "[ 37] Accuracy: 0.165  \t  Loss: 0.767 \n",
      "[ 38] Accuracy: 0.175  \t  Loss: 0.765 \n",
      "[ 39] Accuracy: 0.193  \t  Loss: 0.760 \n",
      "[ 40] Accuracy: 0.198  \t  Loss: 0.761 \n",
      "[ 41] Accuracy: 0.183  \t  Loss: 0.763 \n",
      "[ 42] Accuracy: 0.219  \t  Loss: 0.761 \n",
      "[ 43] Accuracy: 0.169  \t  Loss: 0.769 \n",
      "[ 44] Accuracy: 0.165  \t  Loss: 0.768 \n",
      "[ 45] Accuracy: 0.172  \t  Loss: 0.767 \n",
      "[ 46] Accuracy: 0.204  \t  Loss: 0.766 \n",
      "[ 47] Accuracy: 0.180  \t  Loss: 0.761 \n",
      "[ 48] Accuracy: 0.200  \t  Loss: 0.762 \n",
      "[ 49] Accuracy: 0.220  \t  Loss: 0.759 \n"
     ]
    }
   ],
   "source": [
    "# Training code\n",
    "BS = 8\n",
    "\n",
    "image_data = all_imgs_processed\n",
    "label_data = all_labels\n",
    "print(image_data.shape, label_data.shape)\n",
    "\n",
    "training = vgg_obj.training\n",
    "keep_prob = vgg_obj.keep_prob\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Let's shuffle the data every epoch\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(image_data)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(label_data)\n",
    "    # Go through the entire dataset once\n",
    "    accuracy_vals, loss_vals = [], []\n",
    "    for i in range(0, image_data.shape[0]-BS+1, BS):\n",
    "        # Train a single batch\n",
    "        batch_images, batch_labels = image_data[i:i+BS], label_data[i:i+BS]\n",
    "        accuracy_val, loss_val, _ = sess.run([accuracy, loss, opt], feed_dict={imgs: batch_images, labels_ph: batch_labels, training: True, keep_prob: 0.9})\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "        loss_vals.append(loss_val)\n",
    "\n",
    "#     val_correct = []\n",
    "#     for i in range(0, image_val.shape[0], BS):\n",
    "#         batch_images, batch_labels = image_val[i:i+BS], label_val[i:i+BS]\n",
    "#         val_correct.extend( sess.run(correct, feed_dict={eval_inputs: batch_images, labels_ph: batch_labels}) )\n",
    "    print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f '%(epoch, np.mean(accuracy_vals), np.mean(loss_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Michael Guerzhoy and Davi Frossard, 2016\n",
    "#AlexNet implementation in TensorFlow, with weights\n",
    "#Details: \n",
    "#http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n",
    "#\n",
    "#With code from https://github.com/ethereon/caffe-tensorflow\n",
    "#Model from  https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet\n",
    "#Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow\n",
    "#\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "from numpy import *\n",
    "import os\n",
    "#from pylab import *\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from caffe_classes import class_names\n",
    "\n",
    "train_x = zeros((1, 100,100,3)).astype(float32)\n",
    "train_y = zeros((1, 15))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#Read Image, and change to BGR\n",
    "\n",
    "\n",
    "im1 = (imread(\"laska.png\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "im1[:, :, 0], im1[:, :, 2] = im1[:, :, 2], im1[:, :, 0]\n",
    "\n",
    "im2 = (imread(\"poodle.png\")[:,:,:3]).astype(float32)\n",
    "im2[:, :, 0], im2[:, :, 2] = im2[:, :, 2], im2[:, :, 0]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# (self.feed('data')\n",
    "#         .conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm1')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "#         .conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm2')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "#         .conv(3, 3, 384, 1, 1, name='conv3')\n",
    "#         .conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "#         .conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "#         .fc(4096, name='fc6')\n",
    "#         .fc(4096, name='fc7')\n",
    "#         .fc(1000, relu=False, name='fc8')\n",
    "#         .softmax(name='prob'))\n",
    "\n",
    "#In Python 3.5, change this to:\n",
    "net_data = load(open(\"bvlc_alexnet.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "#net_data = load(\"bvlc_alexnet.npy\").item()\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups =  tf.split(input, group, 3)   #tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(kernel, group, 3)  #tf.split(3, group, kernel) \n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(output_groups, 3)          #tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "print(x)\n",
    "\n",
    "\n",
    "#conv1\n",
    "#conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "#lrn1\n",
    "#lrn(2, 2e-05, 0.75, name='norm1')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool1\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "#conv2\n",
    "#conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "#lrn2\n",
    "#lrn(2, 2e-05, 0.75, name='norm2')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool2\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#conv3\n",
    "#conv(3, 3, 384, 1, 1, name='conv3')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "#conv4\n",
    "#conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "#conv5\n",
    "#conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "#maxpool5\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "alexnet_flattened = tf.contrib.layers.flatten(maxpool5)\n",
    "\n",
    "fc1 = tf.contrib.layers.fully_connected(alexnet_flattened, 4096, activation_fn=tf.nn.relu)\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, 4096, activation_fn=tf.nn.relu)\n",
    "fc3 = tf.contrib.layers.fully_connected(fc2, 15, activation_fn=None)\n",
    "\n",
    "fc8 = fc3\n",
    "\n",
    "################################\n",
    "\n",
    "# #fc6\n",
    "# #fc(4096, name='fc6')\n",
    "# fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "# fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "# fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "# #fc7\n",
    "# #fc(4096, name='fc7')\n",
    "# fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "# fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "# fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "\n",
    "# #fc8\n",
    "# #fc(1000, relu=False, name='fc8')\n",
    "# fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "# fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
    "# fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "#prob\n",
    "#softmax(name='prob'))\n",
    "# output = tf.nn.softmax(fc8)\n",
    "output = tf.identity(fc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up variables for training\n",
    "\n",
    "labels_ph = tf.placeholder(tf.int64, (None), name='labels_ph')\n",
    "labels_ph_oh = tf.one_hot(labels_ph, 15)\n",
    "print(labels_ph_oh)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=labels_ph_oh)\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "opt = optimizer.minimize(loss)\n",
    "\n",
    "print(labels_ph_oh)\n",
    "correct = tf.equal(tf.argmax(output, 1), tf.argmax(labels_ph_oh, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "BS = 32\n",
    "\n",
    "image_data = all_imgs_processed\n",
    "label_data = all_labels\n",
    "print(image_data.shape, label_data.shape)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Let's shuffle the data every epoch\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(image_data)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(label_data)\n",
    "    # Go through the entire dataset once\n",
    "    accuracy_vals, loss_vals = [], []\n",
    "    for i in range(0, image_data.shape[0]-BS+1, BS):\n",
    "        # Train a single batch\n",
    "        batch_images, batch_labels = image_data[i:i+BS], label_data[i:i+BS]\n",
    "        accuracy_val, loss_val, _ = sess.run([accuracy, loss, opt], feed_dict={x: batch_images, labels_ph: batch_labels})\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "        loss_vals.append(loss_val)\n",
    "\n",
    "#     val_correct = []\n",
    "#     for i in range(0, image_val.shape[0], BS):\n",
    "#         batch_images, batch_labels = image_val[i:i+BS], label_val[i:i+BS]\n",
    "#         val_correct.extend( sess.run(correct, feed_dict={eval_inputs: batch_images, labels_ph: batch_labels}) )\n",
    "    print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f '%(epoch, np.mean(accuracy_vals), np.mean(loss_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original AlexNet code\n",
    "\n",
    "#Output:\n",
    "\n",
    "\n",
    "# t = time.time()\n",
    "# output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# for input_im_ind in range(output.shape[0]):\n",
    "#     inds = argsort(output)[input_im_ind,:]\n",
    "#     print(\"Image\", input_im_ind)\n",
    "#     for i in range(5):\n",
    "#         print(class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]])\n",
    "\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs_ph = tf.placeholder(tf.float32, (None, 64, 64, 3), name='inputs')\n",
    "\n",
    "labels_ph = tf.placeholder(tf.int64, (None), name='labels')\n",
    "\n",
    "num_classes = 15\n",
    "\n",
    "filters = tf.contrib.layers.conv2d(inputs=inputs_ph, num_outputs=40, kernel_size=[5, 5], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=filters, num_outputs=70, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=120, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=170, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=num_classes + 1, kernel_size=[4, 4], stride=2, padding='same', activation_fn=None)\n",
    "\n",
    "h = tf.contrib.layers.flatten(h)\n",
    "\n",
    "print(h)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=labels_ph))\n",
    "optimizer = tf.train.AdamOptimizer(0.0005, 0.9, 0.999)\n",
    "opt = optimizer.minimize(loss)\n",
    "# correct = tf.equal(tf.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 80\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_imgs_ary = np.expand_dims(imgs_array, axis=3).astype(np.float32)\n",
    "print(input_imgs_ary.dtype)\n",
    "\n",
    "labels_array = labels_array.astype(np.int64)\n",
    "for epoch in range(30):\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(input_imgs_ary)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(labels_array)\n",
    "    loss_vals = []\n",
    "    for i in range(0, input_imgs_ary.shape[0]-BS+1, BS):\n",
    "        # Train a single batch\n",
    "        batch_images, batch_labels = input_imgs_ary[i:i+BS], labels_array[i:i+BS]\n",
    "        loss_val, _ = sess.run([loss, opt], feed_dict={inputs_ph: batch_images, labels_ph: batch_labels})\n",
    "#         accuracy_vals.append(accuracy_val)\n",
    "        loss_vals.append(loss_val)\n",
    "    print(np.mean(loss_vals))\n",
    "    \n",
    "    # no validation accuracy yet.\n",
    "#     print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f' %(epoch, np.mean(accuracy_vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the attribute labels on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import data, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images as input. Filter only for images from 10 classes.\n",
    "# HOG detector on entire image\n",
    "# find points from part centers\n",
    "# Bounding box around part centers (size tbd, energy maximization) to create filters\n",
    "# inspiration from DPM paper on size of bounding box.\n",
    "# train each label to be average of bounding box.\n",
    "\n",
    "# train on 90% of images, holdout on 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_foreground(camera, do_show = True, extract_bb = False):\n",
    "    # https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html\n",
    "\n",
    "    w, h = camera.shape[:2]\n",
    "    mask = np.zeros([w, h], np.uint8)\n",
    "    bgdModel = np.zeros((1,65), np.float64)\n",
    "    fgdModel = np.zeros((1,65), np.float64)\n",
    "    rect = (1, 1, w, h)\n",
    "    cv2.grabCut(camera,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    segmented = camera*mask2[:,:,np.newaxis]\n",
    "    \n",
    "    # Return only the bounding box of the foreground\n",
    "    \n",
    "    if not extract_bb:\n",
    "        return segmented\n",
    "    else:\n",
    "        # Get contours (edge image) of the foreground-separated image\n",
    "        # https://docs.opencv.org/3.3.1/dd/d49/tutorial_py_contour_features.html\n",
    "        segmented_edges = cv2.Canny(segmented, 0, 255)\n",
    "        _,thresh = cv2.threshold(segmented_edges,127,255,0)\n",
    "        _,contours,_ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        # find bounding box around the bird (union of all contours points)\n",
    "        min_x, min_y = [9999999999] * 2\n",
    "        max_x, max_y = [0] * 2\n",
    "        for contour in contours:\n",
    "            for coord in contour:\n",
    "                coord = coord[0]\n",
    "                min_x = min(min_x, coord[0])\n",
    "                max_x = max(max_x, coord[0])\n",
    "                min_y = min(min_y, coord[1])\n",
    "                max_y = max(max_y, coord[1])\n",
    "\n",
    "        cropped = segmented[min_y:max_y, min_x:max_x]\n",
    "        if do_show:\n",
    "            plt.imshow(cropped),plt.colorbar(),plt.show()\n",
    "        return cropped\n",
    "\n",
    "# img = extract_foreground('./CUB_200_2011/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping point.\n",
    "# next: write code to extract pre-defined parts from images\n",
    "# the parts should be bounded by maximum energy.\n",
    "# In order to train CNN to classify parts. There are 135 distinct labels.\n",
    "\n",
    "# possible points of comparison: using Andrew Krause's work with randomly\n",
    "# sampling POI in image.\n",
    "# OR using HOG descriptor and dimensionality reduction on unsupervised method\n",
    "# of finding distinct points.\n",
    "\n",
    "# show that there is room to improve\n",
    "# can we bound an improvement accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
