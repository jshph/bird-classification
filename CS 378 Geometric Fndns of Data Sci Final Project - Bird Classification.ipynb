{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the locations of the parts that are labeled on each image, and join them with the address of the corresponding images. This is to be able to train a classifier to distinguish between each `part_id` by looking at that part in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    image_id                                         image_addr   bb_x   bb_y  \\\n",
      "0          1  001.Black_footed_Albatross/Black_Footed_Albatr...   60.0   27.0   \n",
      "1          2  001.Black_footed_Albatross/Black_Footed_Albatr...  139.0   30.0   \n",
      "2          3  001.Black_footed_Albatross/Black_Footed_Albatr...   14.0  112.0   \n",
      "3          4  001.Black_footed_Albatross/Black_Footed_Albatr...  112.0   90.0   \n",
      "4          5  001.Black_footed_Albatross/Black_Footed_Albatr...   70.0   50.0   \n",
      "5          6  001.Black_footed_Albatross/Black_Footed_Albatr...   33.0   53.0   \n",
      "6          7  001.Black_footed_Albatross/Black_Footed_Albatr...    7.0   75.0   \n",
      "7          8  001.Black_footed_Albatross/Black_Footed_Albatr...   78.0   86.0   \n",
      "8          9  001.Black_footed_Albatross/Black_Footed_Albatr...  112.0   76.0   \n",
      "9         10  001.Black_footed_Albatross/Black_Footed_Albatr...   27.0    4.0   \n",
      "10        11  001.Black_footed_Albatross/Black_Footed_Albatr...    1.0   52.0   \n",
      "11        12  001.Black_footed_Albatross/Black_Footed_Albatr...   47.0   21.0   \n",
      "12        13  001.Black_footed_Albatross/Black_Footed_Albatr...   36.0   32.0   \n",
      "13        14  001.Black_footed_Albatross/Black_Footed_Albatr...   37.0   40.0   \n",
      "14        15  001.Black_footed_Albatross/Black_Footed_Albatr...   23.0   80.0   \n",
      "15        16  001.Black_footed_Albatross/Black_Footed_Albatr...  141.0   87.0   \n",
      "16        17  001.Black_footed_Albatross/Black_Footed_Albatr...   40.0   38.0   \n",
      "17        18  001.Black_footed_Albatross/Black_Footed_Albatr...   45.0   58.0   \n",
      "18        19  001.Black_footed_Albatross/Black_Footed_Albatr...  135.0   83.0   \n",
      "19        20  001.Black_footed_Albatross/Black_Footed_Albatr...   23.0  195.0   \n",
      "20        21  001.Black_footed_Albatross/Black_Footed_Albatr...   11.0   77.0   \n",
      "21        22  001.Black_footed_Albatross/Black_Footed_Albatr...    5.0    4.0   \n",
      "22        23  001.Black_footed_Albatross/Black_Footed_Albatr...  154.0   93.0   \n",
      "23        24  001.Black_footed_Albatross/Black_Footed_Albatr...   50.0   25.0   \n",
      "24        25  001.Black_footed_Albatross/Black_Footed_Albatr...  172.0  106.0   \n",
      "25        26  001.Black_footed_Albatross/Black_Footed_Albatr...   37.0   61.0   \n",
      "26        27  001.Black_footed_Albatross/Black_Footed_Albatr...   65.0   62.0   \n",
      "27        28  001.Black_footed_Albatross/Black_Footed_Albatr...    1.0    2.0   \n",
      "28        29  001.Black_footed_Albatross/Black_Footed_Albatr...   97.0    3.0   \n",
      "29        30  001.Black_footed_Albatross/Black_Footed_Albatr...   90.0   29.0   \n",
      "\n",
      "     bb_w   bb_h  class_id  \n",
      "0   325.0  304.0         1  \n",
      "1   153.0  264.0         1  \n",
      "2   388.0  186.0         1  \n",
      "3   255.0  242.0         1  \n",
      "4   134.0  303.0         1  \n",
      "5   251.0  395.0         1  \n",
      "6   420.0  262.0         1  \n",
      "7   333.0  158.0         1  \n",
      "8   221.0  189.0         1  \n",
      "9   199.0  201.0         1  \n",
      "10  342.0  260.0         1  \n",
      "11  179.0  249.0         1  \n",
      "12  197.0  457.0         1  \n",
      "13  262.0  184.0         1  \n",
      "14  395.0  237.0         1  \n",
      "15  256.0  211.0         1  \n",
      "16  254.0  209.0         1  \n",
      "17  254.0  249.0         1  \n",
      "18  304.0  181.0         1  \n",
      "19  255.0  122.0         1  \n",
      "20  281.0  139.0         1  \n",
      "21  210.0  130.0         1  \n",
      "22  188.0  165.0         1  \n",
      "23  156.0  139.0         1  \n",
      "24  265.0  214.0         1  \n",
      "25  409.0  180.0         1  \n",
      "26  309.0  345.0         1  \n",
      "27  499.0  286.0         1  \n",
      "28  235.0  254.0         1  \n",
      "29  224.0  211.0         1  \n"
     ]
    }
   ],
   "source": [
    "# read in part locations\n",
    "part_locs = pd.read_csv('CUB_200_2011/CUB_200_2011/parts/part_locs.txt', sep=' ', header=None)\n",
    "part_locs.columns = ['image_id', 'part_id', 'x', 'y', 'visible']\n",
    "\n",
    "# read in image ids and their addresses\n",
    "im_id_addrs = pd.read_csv('CUB_200_2011/CUB_200_2011/images.txt', sep=' ', header=None)\n",
    "im_id_addrs.columns = ['image_id', 'image_addr']\n",
    "\n",
    "im_addr_parts = im_id_addrs\n",
    "# join part locations and image id dataframes in order to get (part, image address) mappings\n",
    "# im_addr_parts = pd.merge(part_locs, im_id_addrs, on='image_id')\n",
    "\n",
    "# read in bounding boxes per image id\n",
    "im_bbs = pd.read_csv('CUB_200_2011/CUB_200_2011/bounding_boxes.txt', sep=' ', header=None)\n",
    "im_bbs.columns = ['image_id', 'bb_x', 'bb_y', 'bb_w', 'bb_h']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_bbs, on='image_id')\n",
    "\n",
    "im_classes = pd.read_csv('CUB_200_2011/CUB_200_2011/image_class_labels.txt', sep=' ', header=None)\n",
    "im_classes.columns = ['image_id', 'class_id']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_classes, on='image_id')\n",
    "\n",
    "# read in part labels manually because parts can be two words\n",
    "# part_labels = []\n",
    "# with open('CUB_200_2011/CUB_200_2011/parts/parts.txt') as part_labels_file:\n",
    "#     for row in part_labels_file:\n",
    "#         sep = row.find(' ')\n",
    "#         part_id, part_name = row[:sep], row[sep + 1:-1]\n",
    "#         part_labels.append({'part_id': int(part_id), 'part_name': part_name})\n",
    "# part_labels = pd.DataFrame.from_dict(part_labels)\n",
    "\n",
    "# join (part, image addresses) with part labels to make part_id's comprehensible\n",
    "# im_addr_parts = pd.merge(im_addr_parts, part_labels, on='part_id')\n",
    "print(im_addr_parts.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0009_34.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0002_55.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0074_59.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0014_89.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0085_92.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0031_100.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0051_796103.jpg\n",
      "extracting foreground for 1 001.Black_footed_Albatross/Black_Footed_Albatross_0010_796097.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-525256273ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mbb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mfg_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_foreground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbb_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbb_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbb_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbb_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extracting foreground for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5646c7dc8e09>\u001b[0m in \u001b[0;36mextract_foreground\u001b[0;34m(camera, do_show, extract_bb)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfgdModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrabCut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbgdModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfgdModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGC_INIT_WITH_RECT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmask2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msegmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display the parts on the images\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "DISPLAY = False\n",
    "NUM_CLASSES = 15 # limit classes trained\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from collections import defaultdict\n",
    "\n",
    "# GENERATE THE TRAINING DATA\n",
    "\n",
    "# CLASS_IMGS = defaultdict(list)\n",
    "# first level key = class_id. second level key = part_id\n",
    "\n",
    "class_ids = set()\n",
    "import pickle\n",
    "\n",
    "CLASS_INSTANCES = []\n",
    "prev_class_id = 0\n",
    "\n",
    "im_addr_classes = im_addr_parts\n",
    "for index, row in im_addr_parts.iterrows():\n",
    "    im_id, im_addr, bb_x, bb_y, bb_w, bb_h, class_id = row\n",
    "    \n",
    "    if class_id not in class_ids:\n",
    "        if len(class_ids) >= NUM_CLASSES:\n",
    "            break\n",
    "        class_ids.add(class_id)\n",
    "        if prev_class_id != 0:\n",
    "            filename = 'cropped/class_' + str(prev_class_id) + '_fg_np'\n",
    "            pickle.dump(CLASS_INSTANCES, open(filename, 'wb'))\n",
    "            CLASS_INSTANCES = []\n",
    "            print('Wrote to file for class', class_id)\n",
    "        prev_class_id = class_id\n",
    "    \n",
    "    if not visible:\n",
    "        continue\n",
    "    \n",
    "    img = cv2.imread('CUB_200_2011/CUB_200_2011/images/' + im_addr)\n",
    "    \n",
    "    bb_x, bb_y, bb_w, bb_h = [int(x) for x in [bb_x, bb_y, bb_w, bb_h]]\n",
    "    fg_img = extract_foreground(img[bb_y:bb_y + bb_h, bb_x:bb_x + bb_w, :])\n",
    "    print('extracting foreground for', class_id, im_addr)\n",
    "    \n",
    "    CLASS_INSTANCES.append(fg_img)\n",
    "    \n",
    "    \n",
    "#     for scale in range(5, 21, 5):\n",
    "#         x, y = int(x), int(y)\n",
    "#         part_crop = img[x - scale:x + scale, y - scale:y + scale]\n",
    "#         if 0 in part_crop.shape:\n",
    "#             continue\n",
    "#         resized = transform.resize(part_crop, (20, 20))\n",
    "#         PARTS[class_id][part_id].append(resized)\n",
    "    \n",
    "#     if DISPLAY:\n",
    "#         ax = fig.add_subplot(5, 5, counter)\n",
    "#         ax.imshow(img)\n",
    "#         bb = patches.Rectangle((bb_x, bb_y), bb_w, bb_h, fill=False)\n",
    "#         ax.add_patch(bb)\n",
    "#         ax.plot(x, y, '+', linewidth=100, color='red')\n",
    "\n",
    "#     for scale in range(5, 40, 5):\n",
    "#         filt = patches.Rectangle((x - scale, y - scale), scale * 2, scale * 2, fill=False)\n",
    "#         if DISPLAY:\n",
    "#             ax.add_patch(filt)\n",
    "\n",
    "# CLASS_IMGS = dict(CLASS_IMGS)\n",
    "\n",
    "# for class_id in CLASS_IMGS.keys():\n",
    "#     CLASS_IMGS[class_id] = np.array(CLASS_IMGS[class_id])\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(CLASS_IMGS, open('training_patches', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "PARTS = pickle.load(open('training_patches', 'rb'))\n",
    "# PARTS: {class_id: {part_id: [img, ...]}...}\n",
    "\n",
    "# reshape the parts into something that tensorflow will process\n",
    "\n",
    "imgs_array = np.empty(shape=(0, 20, 20))\n",
    "labels_array = np.zeros(shape=(0))\n",
    "for class_id, parts in PARTS.items():\n",
    "    for part_id, part_ary in parts.items():\n",
    "        imgs_array = np.concatenate((imgs_array, part_ary), axis=0)\n",
    "        # create labels array of same length as part_ary (# of images for that part)\n",
    "        part_labels = [part_id] * part_ary.shape[0] # [15 * (class_id - 1) + part_id] * part_ary.shape[0]\n",
    "        labels_array = np.concatenate((labels_array, part_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs_ph = tf.placeholder(tf.float32, (None, 20, 20, 1), name='inputs')\n",
    "\n",
    "labels_ph = tf.placeholder(tf.int64, (None), name='labels')\n",
    "\n",
    "num_classes = 15 # * 15\n",
    "\n",
    "h = tf.contrib.layers.conv2d(inputs=inputs_ph, num_outputs=20, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=70, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=120, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=170, kernel_size=[4, 4], stride=2, padding='same', activation_fn=tf.nn.relu)\n",
    "h = tf.contrib.layers.conv2d(inputs=h, num_outputs=num_classes + 1, kernel_size=[4, 4], stride=2, padding='same', activation_fn=None)\n",
    "\n",
    "h = tf.contrib.layers.flatten(h)\n",
    "\n",
    "print(h)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=labels_ph))\n",
    "optimizer = tf.train.AdamOptimizer(0.0005, 0.9, 0.999)\n",
    "opt = optimizer.minimize(loss)\n",
    "# correct = tf.equal(tf.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BS = 80\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_imgs_ary = np.expand_dims(imgs_array, axis=3).astype(np.float32)\n",
    "print(input_imgs_ary.dtype)\n",
    "\n",
    "labels_array = labels_array.astype(np.int64)\n",
    "for epoch in range(30):\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(input_imgs_ary)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(labels_array)\n",
    "    loss_vals = []\n",
    "    for i in range(0, input_imgs_ary.shape[0]-BS+1, BS):\n",
    "        # Train a single batch\n",
    "        batch_images, batch_labels = input_imgs_ary[i:i+BS], labels_array[i:i+BS]\n",
    "        loss_val, _ = sess.run([loss, opt], feed_dict={inputs_ph: batch_images, labels_ph: batch_labels})\n",
    "#         accuracy_vals.append(accuracy_val)\n",
    "        loss_vals.append(loss_val)\n",
    "    print(np.mean(loss_vals))\n",
    "    \n",
    "    # no validation accuracy yet.\n",
    "#     print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f' %(epoch, np.mean(accuracy_vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the attribute labels on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_labels_raw = pd.read_csv('CUB_200_2011/CUB_200_2011/attributes/image_attribute_labels.txt', \\\n",
    "                               sep=' ', header=None, error_bad_lines=False)\n",
    "image_labels_raw.columns = ['image_id', 'attribute_id', 'is_present', 'certainty_id', 'time']\n",
    "print(image_labels_raw.head(5))\n",
    "num_attributes = image_labels_raw.attribute_id.max()\n",
    "num_images = image_labels_raw.image_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding for `image_labels_raw`. Each image can have several of many attributes, but this is sparse. For example, an image of a bird can have \"pointy beak\", \"blue belly\", but not \"white tufts\". Each of these labels is associated with an integer id, `attribute_id` column as shown in the output of the previous cell. This embedding's rows represent the `image_id`, and the columns are `0` if the `attribute_id` is not present for that image, and `1` if it is present.\n",
    "\n",
    "The cell below runs for 3.67M rows, which takes awhile to run. Then, it's saved to disk so that it can be loaded quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_labels = np.zeros((num_images, num_attributes))\n",
    "for idx, row in image_labels_raw.iterrows():\n",
    "    im_id = int(row['image_id']) - 1\n",
    "    attr_id = int(row['attribute_id']) - 1\n",
    "    if row['is_present'] == 1.0:\n",
    "        image_labels[im_id][attr_id] = 1\n",
    "    if idx % 50000 == 0:\n",
    "        print(idx, end=' ')\n",
    "np.save('im_label_embedding.npy', image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_labels = np.load('im_label_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import data, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "camera = data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd, hog_image = hog(camera, orientations=8, pixels_per_cell=(8, 8), \\\n",
    "                   cells_per_block=(1,1), visualise=True)\n",
    "\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(hog_image_rescaled.shape)\n",
    "plt.imshow(hog_image_rescaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read images as input. Filter only for images from 10 classes.\n",
    "# HOG detector on entire image\n",
    "# find points from part centers\n",
    "# Bounding box around part centers (size tbd, energy maximization) to create filters\n",
    "# inspiration from DPM paper on size of bounding box.\n",
    "# train each label to be average of bounding box.\n",
    "\n",
    "# train on 90% of images, holdout on 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_foreground(camera, do_show = True, extract_bb = False):\n",
    "    # https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html\n",
    "\n",
    "    w, h = camera.shape[:2]\n",
    "    mask = np.zeros([w, h], np.uint8)\n",
    "    bgdModel = np.zeros((1,65), np.float64)\n",
    "    fgdModel = np.zeros((1,65), np.float64)\n",
    "    rect = (1, 1, w, h)\n",
    "    cv2.grabCut(camera,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    segmented = camera*mask2[:,:,np.newaxis]\n",
    "    \n",
    "    # Return only the bounding box of the foreground\n",
    "    \n",
    "    if not extract_bb:\n",
    "        return segmented\n",
    "    else:\n",
    "        # Get contours (edge image) of the foreground-separated image\n",
    "        # https://docs.opencv.org/3.3.1/dd/d49/tutorial_py_contour_features.html\n",
    "        segmented_edges = cv2.Canny(segmented, 0, 255)\n",
    "        _,thresh = cv2.threshold(segmented_edges,127,255,0)\n",
    "        _,contours,_ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        # find bounding box around the bird (union of all contours points)\n",
    "        min_x, min_y = [9999999999] * 2\n",
    "        max_x, max_y = [0] * 2\n",
    "        for contour in contours:\n",
    "            for coord in contour:\n",
    "                coord = coord[0]\n",
    "                min_x = min(min_x, coord[0])\n",
    "                max_x = max(max_x, coord[0])\n",
    "                min_y = min(min_y, coord[1])\n",
    "                max_y = max(max_y, coord[1])\n",
    "\n",
    "        cropped = segmented[min_y:max_y, min_x:max_x]\n",
    "        if do_show:\n",
    "            plt.imshow(cropped),plt.colorbar(),plt.show()\n",
    "        return cropped\n",
    "\n",
    "# img = extract_foreground('./CUB_200_2011/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopping point.\n",
    "# next: write code to extract pre-defined parts from images\n",
    "# the parts should be bounded by maximum energy.\n",
    "# In order to train CNN to classify parts. There are 135 distinct labels.\n",
    "\n",
    "# possible points of comparison: using Andrew Krause's work with randomly\n",
    "# sampling POI in image.\n",
    "# OR using HOG descriptor and dimensionality reduction on unsupervised method\n",
    "# of finding distinct points.\n",
    "\n",
    "# show that there is room to improve\n",
    "# can we bound an improvement accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
