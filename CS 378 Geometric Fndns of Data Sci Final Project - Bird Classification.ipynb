{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the locations of the parts that are labeled on each image, and join them with the address of the corresponding images. This is to be able to train a classifier to distinguish between each `part_id` by looking at that part in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in part locations\n",
    "part_locs = pd.read_csv('CUB_200_2011/CUB_200_2011/parts/part_locs.txt', sep=' ', header=None)\n",
    "part_locs.columns = ['image_id', 'part_id', 'x', 'y', 'visible']\n",
    "\n",
    "# read in image ids and their addresses\n",
    "im_id_addrs = pd.read_csv('CUB_200_2011/CUB_200_2011/images.txt', sep=' ', header=None)\n",
    "im_id_addrs.columns = ['image_id', 'image_addr']\n",
    "\n",
    "# join part locations and image id dataframes in order to get (part, image address) mappings\n",
    "im_addr_parts = pd.merge(part_locs, im_id_addrs, on='image_id')\n",
    "\n",
    "# read in bounding boxes per image id\n",
    "im_bbs = pd.read_csv('CUB_200_2011/CUB_200_2011/bounding_boxes.txt', sep=' ', header=None)\n",
    "im_bbs.columns = ['image_id', 'bb_x', 'bb_y', 'bb_w', 'bb_h']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_bbs, on='image_id')\n",
    "\n",
    "im_classes = pd.read_csv('CUB_200_2011/CUB_200_2011/image_class_labels.txt', sep=' ', header=None)\n",
    "im_classes.columns = ['image_id', 'class_id']\n",
    "\n",
    "im_addr_parts = pd.merge(im_addr_parts, im_classes, on='image_id')\n",
    "\n",
    "# read in part labels manually because parts can be two words\n",
    "part_labels = []\n",
    "with open('CUB_200_2011/CUB_200_2011/parts/parts.txt') as part_labels_file:\n",
    "    for row in part_labels_file:\n",
    "        sep = row.find(' ')\n",
    "        part_id, part_name = row[:sep], row[sep + 1:-1]\n",
    "        part_labels.append({'part_id': int(part_id), 'part_name': part_name})\n",
    "part_labels = pd.DataFrame.from_dict(part_labels)\n",
    "\n",
    "# join (part, image addresses) with part labels to make part_id's comprehensible\n",
    "im_addr_parts = pd.merge(im_addr_parts, part_labels, on='part_id')\n",
    "# print(im_addr_parts.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuapham/.virtualenvs/cv3/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/joshuapham/.virtualenvs/cv3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on class 1\n",
      "Operating on class 2\n",
      "Operating on class 3\n",
      "Operating on class 4\n",
      "Operating on class 5\n",
      "Operating on class 6\n",
      "Operating on class 7\n",
      "Operating on class 8\n"
     ]
    }
   ],
   "source": [
    "# display the parts on the images\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "DISPLAY = False\n",
    "NUM_CLASSES = 15 # limit classes trained\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from collections import defaultdict\n",
    "\n",
    "PARTS = defaultdict(lambda: defaultdict(list))\n",
    "# first level key = class_id. second level key = part_id\n",
    "\n",
    "class_ids = set()\n",
    "\n",
    "im_addr_parts = im_addr_parts.sort_values('class_id')\n",
    "for index, row in im_addr_parts.iterrows():\n",
    "    im_id, part_id, x, y, visible, im_addr, bb_x, bb_y, bb_w, bb_h, class_id, part_name = row\n",
    "    \n",
    "    if class_id not in class_ids:\n",
    "        if len(class_ids) >= NUM_CLASSES:\n",
    "            break\n",
    "        class_ids.add(class_id)\n",
    "        print('Operating on class', class_id)\n",
    "    \n",
    "    if not visible:\n",
    "        continue\n",
    "    \n",
    "    img = io.imread('CUB_200_2011/CUB_200_2011/images/' + im_addr, as_grey=True)\n",
    "    \n",
    "    for scale in range(5, 21, 5):\n",
    "        x, y = int(x), int(y)\n",
    "        part_crop = img[x - scale:x + scale, y - scale:y + scale]\n",
    "        if 0 in part_crop.shape:\n",
    "            continue\n",
    "        resized = transform.resize(part_crop, (20, 20))\n",
    "        PARTS[class_id][part_id].append(resized)\n",
    "    \n",
    "    if DISPLAY:\n",
    "        ax = fig.add_subplot(5, 5, counter)\n",
    "        ax.imshow(img)\n",
    "        bb = patches.Rectangle((bb_x, bb_y), bb_w, bb_h, fill=False)\n",
    "        ax.add_patch(bb)\n",
    "        ax.plot(x, y, '+', linewidth=100, color='red')\n",
    "\n",
    "#     for scale in range(5, 40, 5):\n",
    "#         filt = patches.Rectangle((x - scale, y - scale), scale * 2, scale * 2, fill=False)\n",
    "#         if DISPLAY:\n",
    "#             ax.add_patch(filt)\n",
    "    \n",
    "\n",
    "import pickle\n",
    "pickle.dump(dict(PARTS), open('training_patches', 'wb'))\n",
    "\n",
    "print(len(PARTS[1][1]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the attribute labels on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels_raw = pd.read_csv('CUB_200_2011/CUB_200_2011/attributes/image_attribute_labels.txt', \\\n",
    "                               sep=' ', header=None, error_bad_lines=False)\n",
    "image_labels_raw.columns = ['image_id', 'attribute_id', 'is_present', 'certainty_id', 'time']\n",
    "print(image_labels_raw.head(5))\n",
    "num_attributes = image_labels_raw.attribute_id.max()\n",
    "num_images = image_labels_raw.image_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding for `image_labels_raw`. Each image can have several of many attributes, but this is sparse. For example, an image of a bird can have \"pointy beak\", \"blue belly\", but not \"white tufts\". Each of these labels is associated with an integer id, `attribute_id` column as shown in the output of the previous cell. This embedding's rows represent the `image_id`, and the columns are `0` if the `attribute_id` is not present for that image, and `1` if it is present.\n",
    "\n",
    "The cell below runs for 3.67M rows, which takes awhile to run. Then, it's saved to disk so that it can be loaded quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = np.zeros((num_images, num_attributes))\n",
    "for idx, row in image_labels_raw.iterrows():\n",
    "    im_id = int(row['image_id']) - 1\n",
    "    attr_id = int(row['attribute_id']) - 1\n",
    "    if row['is_present'] == 1.0:\n",
    "        image_labels[im_id][attr_id] = 1\n",
    "    if idx % 50000 == 0:\n",
    "        print(idx, end=' ')\n",
    "np.save('im_label_embedding.npy', image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_labels = np.load('im_label_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import data, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "camera = data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd, hog_image = hog(camera, orientations=8, pixels_per_cell=(8, 8), \\\n",
    "                   cells_per_block=(1,1), visualise=True)\n",
    "\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(hog_image_rescaled.shape)\n",
    "plt.imshow(hog_image_rescaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read images as input. Filter only for images from 10 classes.\n",
    "# HOG detector on entire image\n",
    "# find points from part centers\n",
    "# Bounding box around part centers (size tbd, energy maximization) to create filters\n",
    "# inspiration from DPM paper on size of bounding box.\n",
    "# train each label to be average of bounding box.\n",
    "\n",
    "# train on 90% of images, holdout on 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_foreground(im_path, do_show = True):\n",
    "    # https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html\n",
    "    \n",
    "    import cv2\n",
    "    camera = cv2.imread(im_path)\n",
    "\n",
    "    w, h = camera.shape[:2]\n",
    "    mask = np.zeros([w, h], np.uint8)\n",
    "    bgdModel = np.zeros((1,65), np.float64)\n",
    "    fgdModel = np.zeros((1,65), np.float64)\n",
    "    rect = (1, 1, w, h)\n",
    "    cv2.grabCut(camera,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    segmented = camera*mask2[:,:,np.newaxis]\n",
    "    \n",
    "    # Return only the bounding box of the foreground\n",
    "    \n",
    "    # Get contours (edge image) of the foreground-separated image\n",
    "    # https://docs.opencv.org/3.3.1/dd/d49/tutorial_py_contour_features.html\n",
    "    segmented_edges = cv2.Canny(segmented, 0, 255)\n",
    "    _,thresh = cv2.threshold(segmented_edges,127,255,0)\n",
    "    _,contours,_ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    # find bounding box around the bird (union of all contours points)\n",
    "    min_x, min_y = [9999999999] * 2\n",
    "    max_x, max_y = [0] * 2\n",
    "    for contour in contours:\n",
    "        for coord in contour:\n",
    "            coord = coord[0]\n",
    "            min_x = min(min_x, coord[0])\n",
    "            max_x = max(max_x, coord[0])\n",
    "            min_y = min(min_y, coord[1])\n",
    "            max_y = max(max_y, coord[1])\n",
    "    \n",
    "    cropped = segmented[min_y:max_y, min_x:max_x]\n",
    "    if do_show:\n",
    "        plt.imshow(cropped),plt.colorbar(),plt.show()\n",
    "    return cropped\n",
    "\n",
    "extract_foreground('./CUB_200_2011/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopping point.\n",
    "# next: write code to extract pre-defined parts from images\n",
    "# the parts should be bounded by maximum energy.\n",
    "# In order to train CNN to classify parts. There are 135 distinct labels.\n",
    "\n",
    "# possible points of comparison: using Andrew Krause's work with randomly\n",
    "# sampling POI in image.\n",
    "# OR using HOG descriptor and dimensionality reduction on unsupervised method\n",
    "# of finding distinct points.\n",
    "\n",
    "# show that there is room to improve\n",
    "# can we bound an improvement accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
