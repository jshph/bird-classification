{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (1447, 224, 224, 3)\n",
      "Validation set size: (670, 224, 224, 3)\n",
      "Test set size: (675, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# NUM_CLASSES = 50 # limit classes trained\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.misc import imread, imresize\n",
    "from numpy.random import choice\n",
    "\n",
    "# used to help generate training, testing, and validation datasets by randomly sampling all data.\n",
    "class Datasets(object):\n",
    "    def __init__(self, ratio, subdir=\"default\", num_classes=50, load_new_data=True):\n",
    "        \"\"\"\n",
    "        Ratio: [training, validation, testing] ratios.\n",
    "        load_new_data: whether or not to create new train/val/test datasets from scratch\n",
    "        \"\"\"\n",
    "        self.sets = {}\n",
    "        self.ordered_keys = ['train', 'val', 'test']\n",
    "        self.num_classes = num_classes\n",
    "        self.meta = pickle.load(open('metadata', 'rb'))\n",
    "\n",
    "        for set_key in ['train', 'val', 'test']:\n",
    "            self.sets[set_key] = {}\n",
    "            self.sets[set_key]['imgs'] = []\n",
    "            self.sets[set_key]['lbls'] = []\n",
    "            self.sets[set_key]['addrs'] = []\n",
    "\n",
    "        if load_new_data:\n",
    "            all_instances = self.load_all_instances()\n",
    "            self.spread_data(ratio, all_instances)\n",
    "            self.save_addrs(subdir)\n",
    "        else:\n",
    "            self.load_from_addrs(subdir)\n",
    "    \n",
    "    def load_single_img(self, bb_coords, im_addr):\n",
    "        \"\"\"\n",
    "        bb_coords = takes string values for x, y, width, and height of bounding box.\n",
    "        im_addr = filepath of the image relative to CUB_200_2011/images/\n",
    "        \"\"\"\n",
    "        img = imread('CUB_200_2011/images/' + im_addr)\n",
    "\n",
    "        bb_x, bb_y, bb_w, bb_h = [int(x) for x in bb_coords]\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.dstack([img, img, img])\n",
    "        if 0 in img.shape:\n",
    "            raise\n",
    "\n",
    "        img = img[bb_y:bb_y + bb_h, bb_x:bb_x + bb_w, :]\n",
    "        img = imresize(img, (224, 224))\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def load_all_instances(self):\n",
    "        \"\"\"\n",
    "        Loads in all images and their classes, before creating train/val/test datasets with them.\n",
    "        Returns: {class_id -> [all instances of this class, as processed images]}\n",
    "        \"\"\"\n",
    "        class_ids = set()\n",
    "\n",
    "        ALL_INSTANCES = defaultdict(list) # maps class id to list of images in that class.\n",
    "\n",
    "        for index, row in self.meta.iterrows():\n",
    "            im_id, im_addr, bb_x, bb_y, bb_w, bb_h, class_id = row\n",
    "\n",
    "            if class_id not in class_ids:\n",
    "                if len(class_ids) >= self.num_classes:\n",
    "                    break\n",
    "                class_ids.add(class_id)\n",
    "            \n",
    "            try:\n",
    "                img = self.load_single_img([bb_x, bb_y, bb_w, bb_h], im_addr)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            ALL_INSTANCES[class_id] += [(img, im_addr)]\n",
    "        \n",
    "        return dict(ALL_INSTANCES)\n",
    "    \n",
    "    def save_addrs(self, subdir):\n",
    "        \"\"\"\n",
    "        Only run for the case of creating new train/val/test datasets: saves addresses/classes to file, so that\n",
    "        clearing output and restarting will not change the dataset makeup.\n",
    "        \"\"\"\n",
    "        for set_type in self.ordered_keys:\n",
    "            with open('./dataset_reconstruct/' + subdir + '/' + set_type + '.txt', 'w') as outf:\n",
    "                for lbl, addr in zip(self.sets[set_type]['lbls'], self.sets[set_type]['addrs']):\n",
    "#                     print(set_type, lbl, addr)\n",
    "                    outf.write(lbl.astype(str) + ' ' + addr.astype(str) + '\\n')\n",
    "    \n",
    "    def load_from_addrs(self, subdir):\n",
    "        \"\"\"\n",
    "        subdir = subdirectory under `dataset_reconstruct`\n",
    "        Loads dataset from lists of image addresses: {train, val, test}.txt\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for set_type in self.ordered_keys:\n",
    "            with open('./dataset_reconstruct/' + subdir + '/' + set_type + '.txt', 'r') as inf:\n",
    "                for line in inf:\n",
    "                    class_id, im_addr = line.split()\n",
    "                    for row in self.meta.loc[self.meta['image_addr'] == im_addr].iterrows():\n",
    "                        _, _, bb_x, bb_y, bb_w, bb_h, _ = row[1]\n",
    "                    \n",
    "                    try:\n",
    "                        img = self.load_single_img([bb_x, bb_y, bb_w, bb_h], im_addr)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    self.sets[set_type]['imgs'].append(img)\n",
    "                    self.sets[set_type]['lbls'].append(class_id)\n",
    "\n",
    "            self.sets[set_type]['imgs'] = np.array(self.sets[set_type]['imgs'])\n",
    "            self.sets[set_type]['lbls'] = np.array(self.sets[set_type]['lbls'])\n",
    "\n",
    "    def spread_data(self, ratio, all_instances):\n",
    "        \"\"\"\n",
    "        Forms training, validation, and testing set by random sampling\n",
    "        \"\"\"\n",
    "        for class_id, imgs in all_instances.items():\n",
    "        # distributes samples evenly from each class, to balance out train/test/validation sets.\n",
    "            train_s, train_e = 0, round(ratio[0] * len(imgs))\n",
    "            val_s, val_e = round(ratio[0] * len(imgs)) + 1, round((ratio[0] + ratio[1]) * len(imgs))\n",
    "            test_s, test_e = round((ratio[0] + ratio[1]) * len(imgs) + 1), len(imgs)\n",
    "            partitioned_imgs = [imgs[train_s:train_e], imgs[val_s:val_e], imgs[test_s:test_e]]\n",
    "            \n",
    "            for idx, partition in enumerate(partitioned_imgs):\n",
    "                for img, img_addr in partition:\n",
    "#                     set_choice_idx = choice([0, 1, 2], p=ratio)\n",
    "                    set_type = self.ordered_keys[idx]\n",
    "                    self.sets[set_type]['imgs'].append(img)\n",
    "                    self.sets[set_type]['lbls'].append(class_id)\n",
    "                    self.sets[set_type]['addrs'].append(img_addr)\n",
    "        for set_type in self.ordered_keys:\n",
    "            self.sets[set_type]['imgs'] = np.array(self.sets[set_type]['imgs'])\n",
    "            self.sets[set_type]['lbls'] = np.array(self.sets[set_type]['lbls'])\n",
    "            self.sets[set_type]['addrs'] = np.array(self.sets[set_type]['addrs'])\n",
    "\n",
    "    def show_dataset_size(self):\n",
    "        \"\"\"\n",
    "        Print length of training, validation, and testing set\n",
    "        \"\"\"\n",
    "        print('Train set size:', self.sets['train']['imgs'].shape)\n",
    "        print('Validation set size:', self.sets['val']['imgs'].shape)\n",
    "        print('Test set size:', self.sets['test']['imgs'].shape)\n",
    "\n",
    "train_test_ratio = [0.5, 0.25, 0.25]\n",
    "\n",
    "datasets = Datasets(train_test_ratio)\n",
    "datasets.show_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset (train, validation, test) from im_addrs stored\n",
    "\n",
    "# datasets = Datasets(train_test_ratio, load_new_data=False)\n",
    "# datasets.show_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n"
     ]
    }
   ],
   "source": [
    "import vgg as vgg_mod\n",
    "import importlib\n",
    "importlib.reload(vgg_mod)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "vgg_obj = None\n",
    "\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg_obj = vgg_mod.vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "\n",
    "labels_ph = tf.placeholder(tf.int64, (None), name='labels_ph')\n",
    "labels_ph_oh = tf.one_hot(labels_ph, 50)\n",
    "\n",
    "# Get trainable variables for the last three custom FC layers (featuring dropout, etc)\n",
    "fc_stem = 'fully_connected'\n",
    "trainable_vars_list = []\n",
    "for i in ['', '_1', '_2']:\n",
    "    trainable_weight_var = tf.contrib.framework.get_variables(fc_stem + i + '/weights:0')\n",
    "    trainable_bias_var = tf.contrib.framework.get_variables(fc_stem + i + '/biases:0')\n",
    "    trainable_vars_list += [trainable_weight_var, trainable_bias_var]\n",
    "\n",
    "optimizer_warm = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "loss_warm = tf.nn.sigmoid_cross_entropy_with_logits(logits=vgg_obj.logits, labels=labels_ph_oh)\n",
    "opt_warm = optimizer_warm.minimize(loss_warm, var_list=trainable_vars_list)\n",
    "\n",
    "optimizer_full = tf.train.AdamOptimizer(0.00001, 0.9, 0.999)\n",
    "loss_full = tf.nn.sigmoid_cross_entropy_with_logits(logits=vgg_obj.logits, labels=labels_ph_oh)\n",
    "opt_full = optimizer_full.minimize(loss_full)\n",
    "\n",
    "correct = tf.equal(tf.argmax(vgg_obj.logits, 1), labels_ph)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vgg_obj.load_weights(vgg_obj.weights, vgg_obj.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] Accuracy: 0.192  \t  Loss: 0.770 \t Val acc: 0.106\n",
      "[  1] Accuracy: 0.486  \t  Loss: 0.728 \t Val acc: 0.248\n",
      "[  2] Accuracy: 0.621  \t  Loss: 0.710 \t Val acc: 0.282\n",
      "[  3] Accuracy: 0.685  \t  Loss: 0.696 \t Val acc: 0.421\n",
      "[  4] Accuracy: 0.744  \t  Loss: 0.687 \t Val acc: 0.364\n",
      "[  5] Accuracy: 0.752  \t  Loss: 0.683 \t Val acc: 0.384\n",
      "[  6] Accuracy: 0.771  \t  Loss: 0.677 \t Val acc: 0.355\n",
      "[  7] Accuracy: 0.763  \t  Loss: 0.679 \t Val acc: 0.349\n",
      "[  8] Accuracy: 0.792  \t  Loss: 0.673 \t Val acc: 0.399\n",
      "[  9] Accuracy: 0.793  \t  Loss: 0.669 \t Val acc: 0.434\n",
      "[ 10] Accuracy: 0.801  \t  Loss: 0.668 \t Val acc: 0.372\n",
      "[ 11] Accuracy: 0.781  \t  Loss: 0.668 \t Val acc: 0.437\n",
      "[ 12] Accuracy: 0.790  \t  Loss: 0.668 \t Val acc: 0.475\n",
      "[ 13] Accuracy: 0.773  \t  Loss: 0.669 \t Val acc: 0.531\n",
      "[ 14] Accuracy: 0.790  \t  Loss: 0.667 \t Val acc: 0.497\n",
      "[  0] Accuracy: 0.783  \t  Loss: 0.668 \t Val acc: 0.531\n",
      "[  1] Accuracy: 0.787  \t  Loss: 0.666 \t Val acc: 0.528\n",
      "[  2] Accuracy: 0.800  \t  Loss: 0.663 \t Val acc: 0.546\n",
      "[  3] Accuracy: 0.808  \t  Loss: 0.663 \t Val acc: 0.549\n",
      "[  4] Accuracy: 0.800  \t  Loss: 0.665 \t Val acc: 0.528\n",
      "[  5] Accuracy: 0.800  \t  Loss: 0.665 \t Val acc: 0.515\n",
      "[  6] Accuracy: 0.800  \t  Loss: 0.664 \t Val acc: 0.533\n",
      "[  7] Accuracy: 0.781  \t  Loss: 0.663 \t Val acc: 0.573\n"
     ]
    }
   ],
   "source": [
    "# Training code\n",
    "BS = 64\n",
    "NUM_EPOCHS_WARM = 15\n",
    "NUM_EPOCHS_FULL = 8\n",
    "\n",
    "train_imgs = datasets.sets['train']['imgs']\n",
    "train_lbls = datasets.sets['train']['lbls']\n",
    "\n",
    "val_imgs = datasets.sets['val']['imgs']\n",
    "val_lbls = datasets.sets['val']['lbls']\n",
    "\n",
    "training = vgg_obj.training\n",
    "keep_prob = vgg_obj.keep_prob\n",
    "\n",
    "mean_train_accs, mean_losses, mean_val_accs = [], [], []\n",
    "\n",
    "def train_for_epochs(optimizer_in, loss_in, epochs, correct_in, accuracy_in):\n",
    "    for epoch in range(epochs):\n",
    "        # Let's shuffle the data every epoch\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(train_imgs)\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(train_lbls)\n",
    "        # Go through the entire dataset once\n",
    "        accuracy_vals, loss_vals, val_correct = [], [], []\n",
    "        for i in range(0, train_imgs.shape[0]-BS+1, BS):\n",
    "            # Train a single batch\n",
    "            batch_images, batch_labels = train_imgs[i:i+BS], train_lbls[i:i+BS]\n",
    "            accuracy_val, loss_val, _ = sess.run([accuracy_in, loss_in, optimizer_in], feed_dict={imgs: batch_images, labels_ph: batch_labels, training: True, keep_prob: 0.9})\n",
    "            accuracy_vals.append(accuracy_val)\n",
    "            loss_vals.append(loss_val)\n",
    "\n",
    "        for i in range(0, val_imgs.shape[0], BS):\n",
    "            batch_images, batch_labels = val_imgs[i:i+BS], val_lbls[i:i+BS]\n",
    "            val_correct.extend( sess.run(correct_in, feed_dict={imgs: batch_images, labels_ph: batch_labels, training: False, keep_prob:0.9}) )\n",
    "        \n",
    "        mean_train_acc, mean_loss, mean_val_acc = np.mean(accuracy_vals), np.mean(loss_vals), np.mean(val_correct)\n",
    "        mean_train_accs.append(mean_train_acc)\n",
    "        mean_losses.append(mean_loss)\n",
    "        mean_val_accs.append(mean_val_acc)\n",
    "        print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f \\t Val acc: %0.3f'%(epoch, mean_train_acc, mean_loss, mean_val_acc))\n",
    "train_for_epochs(opt_warm, loss_warm, NUM_EPOCHS_WARM, correct, accuracy)\n",
    "train_for_epochs(opt_full, loss_full, NUM_EPOCHS_FULL, correct, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes about running hyperparameters:\n",
    "\n",
    "1. Warmup starts to overfit after ~15 epochs. So use 12 instead, and hopefully the full network training will improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 224, 224, 3)\n",
      "Test accuracy:\n",
      "Top-1: 77.6\n",
      "Top-3: 87.8\n",
      "Top-5: 89.8\n"
     ]
    }
   ],
   "source": [
    "test_imgs_processed = datasets.sets['test']['imgs']\n",
    "test_labels = datasets.sets['test']['lbls']\n",
    "print(test_imgs_processed.shape)\n",
    "\n",
    "num_test = len(test_imgs_processed)\n",
    "test_batch_size = 128\n",
    "total_top_1, total_top_3, total_top_5 = [[] for x in range(3)]\n",
    "\n",
    "for batch_start in range(0, num_test - test_batch_size + 1, test_batch_size):\n",
    "    \n",
    "    img_batch = test_imgs_processed[batch_start:batch_start + test_batch_size, :, :, :]\n",
    "#     print(img_batch.shape)\n",
    "    labels_batch = test_labels[batch_start:batch_start + test_batch_size]\n",
    "    test_probabilities = sess.run(vgg_obj.probs, feed_dict={vgg_obj.imgs: img_batch, training: False, keep_prob:0.9})\n",
    "\n",
    "    top_1_correct, top_3_correct, top_5_correct = [0] * 3\n",
    "    for sample_idx, sample in enumerate(test_probabilities):\n",
    "        top_predictions = (np.argsort(sample)[::-1])[:5]\n",
    "        test_label = labels_batch[sample_idx]\n",
    "        if test_label in top_predictions[:1]:\n",
    "            top_1_correct += 1\n",
    "        if test_label in top_predictions[:3]:\n",
    "            top_3_correct += 1\n",
    "        if test_label in top_predictions:\n",
    "            top_5_correct += 1\n",
    "        \n",
    "#     print(top_5_correct)\n",
    "\n",
    "    total_top_1.append(top_1_correct)\n",
    "    total_top_3.append(top_3_correct)\n",
    "    total_top_5.append(top_5_correct)\n",
    "\n",
    "print('Test accuracy:')\n",
    "print('Top-1:', np.mean(total_top_1))\n",
    "print('Top-3:', np.mean(total_top_3))\n",
    "print('Top-5:', np.mean(total_top_5))\n",
    "testaccs = [total_top_1, total_top_3, total_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/bird_573_val_acc.ckpt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, './checkpoints/bird_573_val_acc.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem is file stem, the next three are arrays, and the last is an array of [top 1, top 3, top 5] accuracies.\n",
    "def save_train_acc_loss(stem, trainaccs, losses, valaccs, testaccs):\n",
    "    np.save('./run_results/' + stem + '_accuracy_progress', np.stack([trainaccs, losses, valaccs]))\n",
    "    np.save('./run_results/' + stem + '_test_accs', testaccs)\n",
    "save_train_acc_loss('776testacc', mean_train_accs, mean_losses, mean_val_accs, testaccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191761359572 0.770447969437 0.105970149254\n",
      "0.485795468092 0.728351950645 0.24776119403\n",
      "0.621448874474 0.709888279438 0.282089552239\n",
      "0.684659063816 0.696123301983 0.420895522388\n",
      "0.744318187237 0.686949551105 0.364179104478\n",
      "0.752130687237 0.68284112215 0.383582089552\n",
      "0.771306812763 0.676867246628 0.355223880597\n",
      "0.762784063816 0.679243326187 0.349253731343\n",
      "0.791903436184 0.673110961914 0.398507462687\n",
      "0.793323874474 0.668618381023 0.434328358209\n",
      "0.801136374474 0.668178677559 0.371641791045\n",
      "0.780539751053 0.667970359325 0.437313432836\n",
      "0.79048293829 0.667889773846 0.474626865672\n",
      "0.7734375 0.66898471117 0.531343283582\n",
      "0.789772748947 0.666735053062 0.497014925373\n",
      "0.78267043829 0.667553663254 0.531343283582\n",
      "0.786931812763 0.665761590004 0.528358208955\n",
      "0.800426125526 0.66260445118 0.546268656716\n",
      "0.808238625526 0.662944555283 0.549253731343\n",
      "0.800426125526 0.664998173714 0.528358208955\n",
      "0.799715936184 0.664796948433 0.514925373134\n",
      "0.800426125526 0.663517355919 0.532835820896\n",
      "0.780539751053 0.663491487503 0.573134328358\n"
     ]
    }
   ],
   "source": [
    "stems = ['776testacc']\n",
    "\n",
    "color_sequence = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "                  '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "                  '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "                  '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 7.5))\n",
    "\n",
    "for ax in axs:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    \n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))\n",
    "\n",
    "# ax.set_ylim(0, 0.803)\n",
    "# ax.set_xlim(0, 23)\n",
    "\n",
    "plt.xticks(range(0, 40, 5), fontsize=14)\n",
    "plt.yticks(range(0, 100, 5), fontsize=14)\n",
    "\n",
    "plt.grid(True, 'major', 'y', ls='--', lw=.5, c='k', alpha=.3)\n",
    "\n",
    "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
    "                labelbottom='on', left='off', right='off', labelleft='on')\n",
    "\n",
    "for stem in stems:\n",
    "    a_train_accs, a_losses, a_val_accs = np.load('./run_results/' + stem + '_accuracy_progress.npy')\n",
    "    for t, l, v in zip(a_train_accs, a_losses, a_val_accs):\n",
    "        ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
